{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9e0182-413e-43e9-82fa-3ec4196a07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Import Required Libraries\n",
    "# Install datasets if not already installed\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a291246a-9d3b-4d7c-961e-d5bc79c2f9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5334648-d5cc-48f3-80ae-aff3856ff347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 100 paragraphs.\n",
      "🔹 Sample paragraph:\n",
      " આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the Gujarati Dataset (Streaming Mode)\n",
    "# Load the Gujarati dataset in streaming mode\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=\"guj_Gujr\", streaming=True)\n",
    "\n",
    "# Extract first 100 paragraphs\n",
    "data_list = []\n",
    "for i, item in enumerate(dataset):\n",
    "    data_list.append(item['text'])  # Extract the text field\n",
    "    if i >= 99:\n",
    "        break\n",
    "\n",
    "# ✅ Confirm loading\n",
    "print(f\"✅ Loaded {len(data_list)} paragraphs.\")\n",
    "print(\"🔹 Sample paragraph:\\n\", data_list[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a416b91-cbae-452e-88e7-e5c6e0011bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Tokenized sample: ['મારું', 'ઈમેલ', 'example123@mail.com', 'છે', '.', 'તારીખ', ':', '23/07/2023', 'છે', '.', 'https://abc.com', 'પર', 'જુઓ', '!']\n"
     ]
    }
   ],
   "source": [
    "def sentence_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Splits a paragraph into sentences using Gujarati and standard punctuation.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(?<=[।!?\\.])\\s+')\n",
    "    return sentence_endings.split(text.strip())\n",
    "\n",
    "def word_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence into:\n",
    "    - Emails\n",
    "    - URLs\n",
    "    - Numbers (decimals included)\n",
    "    - Dates (dd/mm/yyyy)\n",
    "    - Words\n",
    "    - Punctuation\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r\"\"\"(\n",
    "            (?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})         |  # Dates\n",
    "            (?:[\\w\\.-]+@[\\w\\.-]+\\.\\w+)               |  # Email addresses\n",
    "            (?:https?://[^\\s]+)                      |  # URLs\n",
    "            (?:\\d+\\.\\d+|\\d+)                         |  # Numbers and decimals\n",
    "            [\\u0A80-\\u0AFF]+                         |  #gujarati words\n",
    "            (?:\\w+)                                  |  # Words\n",
    "            (?:[^\\w\\s])                                 # Punctuation\n",
    "        )\"\"\", re.VERBOSE)\n",
    "    return pattern.findall(sentence)\n",
    "\n",
    "sample_sentence = \"મારું ઈમેલ example123@mail.com છે. તારીખ: 23/07/2023 છે. https://abc.com પર જુઓ!\"\n",
    "print(\"🔹 Tokenized sample:\", word_tokenizer(sample_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85819aaa-5f2b-4b75-ae67-e19e89018264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete. Output saved to 'tokenized_output.txt'\n"
     ]
    }
   ],
   "source": [
    " # Step 4: Tokenize and Save to File\n",
    "# Prepare lists to collect tokens and sentences\n",
    "sentences_all = []\n",
    "words_all = []\n",
    "\n",
    "# Save tokenized output to file\n",
    "with open(\"tokenized_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for para in data_list:\n",
    "        sentences = sentence_tokenizer(para)\n",
    "        for sent in sentences:\n",
    "            tokens = word_tokenizer(sent)\n",
    "            sentences_all.append(sent)\n",
    "            words_all.extend(tokens)\n",
    "            \n",
    "            f.write(\"Sentence: \" + sent.strip() + \"\\n\")\n",
    "            f.write(\"Tokens: \" + \" \".join(tokens) + \"\\n\\n\")\n",
    "\n",
    "print(\"✅ Tokenization complete. Output saved to 'tokenized_output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba6e566-afd8-4a03-aa39-f09c9fce935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Corpus Statistics:\n",
      "Total Sentences: 188\n",
      "Total Words: 2355\n",
      "Total Characters: 10179\n",
      "Average Sentence Length: 12.53 words/sentence\n",
      "Average Word Length: 4.32 characters/word\n",
      "Type/Token Ratio (TTR): 0.5524\n"
     ]
    }
   ],
   "source": [
    " # Step 5: Compute Corpus Statistics# Prepare lists to collect tokens and sentences\n",
    "# Compute statistics\n",
    "total_sentences = len(sentences_all)\n",
    "total_words = len(words_all)\n",
    "total_characters = sum(len(word) for word in words_all)\n",
    "average_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "average_word_length = total_characters / total_words if total_words else 0\n",
    "unique_words = set(words_all)\n",
    "type_token_ratio = len(unique_words) / total_words if total_words else 0\n",
    "\n",
    "# Show results\n",
    "print(\"\\n📊 Corpus Statistics:\")\n",
    "print(f\"Total Sentences: {total_sentences}\")\n",
    "print(f\"Total Words: {total_words}\")\n",
    "print(f\"Total Characters: {total_characters}\")\n",
    "print(f\"Average Sentence Length: {average_sentence_length:.2f} words/sentence\")\n",
    "print(f\"Average Word Length: {average_word_length:.2f} characters/word\")\n",
    "print(f\"Type/Token Ratio (TTR): {type_token_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9683993-062b-4d49-8c50-2d210e6c38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Statistics saved to 'corpus_statistics.txt'\n"
     ]
    }
   ],
   "source": [
    "# step 6 (Optional): Save Stats to File\n",
    "with open(\"corpus_statistics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"📊 Corpus Statistics:\\n\")\n",
    "    f.write(f\"Total Sentences: {total_sentences}\\n\")\n",
    "    f.write(f\"Total Words: {total_words}\\n\")\n",
    "    f.write(f\"Total Characters: {total_characters}\\n\")\n",
    "    f.write(f\"Average Sentence Length: {average_sentence_length:.2f}\\n\")\n",
    "    f.write(f\"Average Word Length: {average_word_length:.2f}\\n\")\n",
    "    f.write(f\"Type/Token Ratio (TTR): {type_token_ratio:.4f}\\n\")\n",
    "\n",
    "print(\"✅ Statistics saved to 'corpus_statistics.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64796d-b4f1-454b-9b3c-179991c1af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read about regular expressions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
