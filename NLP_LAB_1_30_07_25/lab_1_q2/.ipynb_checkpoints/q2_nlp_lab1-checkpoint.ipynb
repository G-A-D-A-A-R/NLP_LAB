{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91849dde-0b11-488e-b07c-8346ac2982e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install & Import Libraries\n",
    "# Install datasets library (if not installed)\n",
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aaf753-2a6d-4de1-9c06-fa76b4f590d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7c3867-f7ca-40ba-b54d-2426ce75b740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found oscar.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the OSCAR 2022 version (open-access version)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moscar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munshuffled_deduplicated_en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Sample a few\u001b[39;00m\n\u001b[1;32m     23\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP_LAB/lib/python3.10/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    988\u001b[0m     )\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found oscar.py"
     ]
    }
   ],
   "source": [
    "#  # Step 2: Load OSCAR-2301 Dataset for a Language\n",
    "# language_code = \"en\"  # Change this to your language: 'hi', 'gu', 'fr', etc.\n",
    "\n",
    "# dataset = load_dataset(\"oscar-corpus/OSCAR-2301\", language_code, split=\"train\", streaming=True)\n",
    "\n",
    "# # Extract first 1000 paragraphs\n",
    "# data_list = []\n",
    "# for i, item in enumerate(dataset):\n",
    "#     data_list.append(item['text'])\n",
    "#     if i >= 999:\n",
    "#         break\n",
    "\n",
    "# # ✅ Confirm loading\n",
    "# print(f\"✅ Loaded {len(data_list)} paragraphs from OSCAR-2301 ({language_code})\")\n",
    "# print(\"🔹 Sample paragraph:\\n\", data_list[0][:300])\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the OSCAR 2022 version (open-access version)\n",
    "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", split=\"train\", streaming=True)\n",
    "\n",
    "# Sample a few\n",
    "data_list = []\n",
    "for i, item in enumerate(dataset):\n",
    "    data_list.append(item[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "\n",
    "print(f\"✅ Loaded {len(data_list)} paragraphs from public OSCAR (English)\")\n",
    "print(\"🔹 Sample paragraph:\\n\", data_list[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d905a6-5acb-4d23-9d6c-791faf24bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3  Sentence tokenizer using ., ?, !, etc.\n",
    "def sentence_tokenizer(text):\n",
    "    sentence_endings = re.compile(r'(?<=[।!?\\.])\\s+')\n",
    "    return sentence_endings.split(text.strip())\n",
    "\n",
    "# Word tokenizer\n",
    "def word_tokenizer(sentence):\n",
    "    pattern = re.compile(\n",
    "        r\"\"\"(\n",
    "            (?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})         |  # Dates\n",
    "            (?:[\\w\\.-]+@[\\w\\.-]+\\.\\w+)               |  # Emails\n",
    "            (?:https?://[^\\s]+)                      |  # URLs\n",
    "            (?:\\d+\\.\\d+|\\d+)                         |  # Numbers\n",
    "            (?:\\w+)                                  |  # Words\n",
    "            (?:[^\\w\\s])                                 # Punctuation\n",
    "        )\"\"\", re.VERBOSE)\n",
    "    return pattern.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35fe8e-306e-4c5f-88b5-0d14a751c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4  Prepare sentence and word lists\n",
    "sentences_all = []\n",
    "words_all = []\n",
    "\n",
    "with open(\"oscar_tokenized_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for para in data_list:\n",
    "        sentences = sentence_tokenizer(para)\n",
    "        for sent in sentences:\n",
    "            tokens = word_tokenizer(sent)\n",
    "            sentences_all.append(sent)\n",
    "            words_all.extend(tokens)\n",
    "\n",
    "            f.write(\"Sentence: \" + sent.strip() + \"\\n\")\n",
    "            f.write(\"Tokens: \" + \" \".join(tokens) + \"\\n\\n\")\n",
    "\n",
    "print(\"✅ Tokenization complete. Output saved to 'oscar_tokenized_output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e518f5-b1cc-40d9-a063-89216943083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compute Corpus Statistics\n",
    "# Total counts\n",
    "total_sentences = len(sentences_all)\n",
    "total_words = len(words_all)\n",
    "total_characters = sum(len(word) for word in words_all)\n",
    "\n",
    "# Averages\n",
    "average_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "average_word_length = total_characters / total_words if total_words else 0\n",
    "\n",
    "# Type/Token Ratio\n",
    "unique_words = set(words_all)\n",
    "type_token_ratio = len(unique_words) / total_words if total_words else 0\n",
    "\n",
    "# Show results\n",
    "print(\"\\n📊 Corpus Statistics:\")\n",
    "print(f\"Total Sentences: {total_sentences}\")\n",
    "print(f\"Total Words: {total_words}\")\n",
    "print(f\"Total Characters: {total_characters}\")\n",
    "print(f\"Average Sentence Length: {average_sentence_length:.2f}\")\n",
    "print(f\"Average Word Length: {average_word_length:.2f}\")\n",
    "print(f\"Type/Token Ratio (TTR): {type_token_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3b00-d561-4df6-ac1d-2a0380f6a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save Stats to File\n",
    "with open(\"oscar_corpus_statistics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"📊 OSCAR Corpus Statistics:\\n\")\n",
    "    f.write(f\"Total Sentences: {total_sentences}\\n\")\n",
    "    f.write(f\"Total Words: {total_words}\\n\")\n",
    "    f.write(f\"Total Characters: {total_characters}\\n\")\n",
    "    f.write(f\"Average Sentence Length: {average_sentence_length:.2f}\\n\")\n",
    "    f.write(f\"Average Word Length: {average_word_length:.2f}\\n\")\n",
    "    f.write(f\"Type/Token Ratio (TTR): {type_token_ratio:.4f}\\n\")\n",
    "\n",
    "print(\"✅ Statistics saved to 'oscar_corpus_statistics.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f89816-e8d2-4c01-b480-533fe39673b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
